CPU part:
In the CPU algorithm, we traverse the array and keep updating the MAX/MIN. 
1. Initialize the MAX/Min as 0. 
2. For each element, we compare it with current MAX/MIN:
    - If this element is greater/smaller, make it MAX/MIN and move to next elemetn;
    - Otherwise, do nothing and move to next elemetn; 
3. After a complete traversal, we can get the correct MAX/MIN of the original array.

GPU part:
In the GPU algorithm, we build GPU_MAX and GPU_MIN kernel each of which is designed for finding MAX/MIN. The general idea is to build a huge binary tree:
1. All of the elements are the leaf nodes of the tree;
2. The local MAX/MIN node between two adjacent nodes can be elevate to their parent node (This is where parallel step is participating in the program);
3. Repeat the step 2 between the adjacent parent nodes and generate greater parent nodes;
4. Stop the repetition when root node occurs, and this node will be the global MAX/MIN;

The detailed implementation of the CUDA kernels is as following:
1. Import elements from global memory into shared memory;
    - Each block import 1024 elements from the global memory;
    - Compare ith element to the (i+512)th element and load the greater/smaller one into the ith cell of shared memory;
    - Initialize the size as 512 and start a loop to do the binary comparison:
        - In each iteration, and for all elements in the range, compare ith element to (i+size/2)th elment, can store the greater/smaller one into the ith cell;
        - Synchronize all the parallel threads;
        - Halve the size;
        - repeat the above three steps until size reach 64;
    - Start another loop similar to the previous loop, but not include the Synchronization, because there are only one warp left (no warp switch):
        - repeat the loop unitl size reaches 1
    - Store the global MAX/MIN of the Kth block to the Kth index cell of the global memory;
2. Change the Cuda block size and thread size declaration and repeat Step 1;
3. Stop at 3rd iteration, here we can get the global MAX/MIN at 1st index cell of the global memory;


Validation:
ug53:~/Desktop/HW2/HW2_submit% ./output 
N: 2M  GPUmax: 199999989  CPUmax: 199999989  GPUtime: 0.000097  CPUtime: 0.004407  SpeedUp: 45.584515   GPUmin: 171  CPUmin: 171  GPUtime: 0.000096  CPUtime: 0.004332  SpeedUp: 44.921810
N: 8M  GPUmax: 199999939  CPUmax: 199999939  GPUtime: 0.000324  CPUtime: 0.017478  SpeedUp: 53.989376   GPUmin: 1  CPUmin: 1  GPUtime: 0.000324  CPUtime: 0.017451  SpeedUp: 53.921909
N: 32M  GPUmax: 199999996  CPUmax: 199999996  GPUtime: 0.001194  CPUtime: 0.070441  SpeedUp: 59.011677   GPUmin: 5  CPUmin: 5  GPUtime: 0.001128  CPUtime: 0.069835  SpeedUp: 61.932598

